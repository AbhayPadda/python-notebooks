{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_data_1 = pd.read_csv(get_object_storage_file_with_credentials_2659b32ff9a04774afc2ee0815088bcf('project1', 'twitter_train.csv'))\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 3)\n",
      "label\n",
      "0    29720\n",
      "1     2242\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_data_1.shape)\n",
    "\n",
    "print(df_data_1.groupby(by=[\"label\"])['label'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from functools import lru_cache\n",
    "from nltk.tag.perceptron import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Load tagger pickle\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "# Lookup if tag is noun, verb, adverb or an adjective\n",
    "tags = {'N': wn.NOUN, 'V': wn.VERB, 'R': wn.ADV, 'J': wn.ADJ}\n",
    "\n",
    "# Memoization of POS tagging and Lemmatizer\n",
    "lemmatize_mem = lru_cache(maxsize=10000)(wnl.lemmatize)\n",
    "tagger_mem = lru_cache(maxsize=10000)(tagger.tag)\n",
    "\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "modified_stopwords = []\n",
    "for idx, stop_word in enumerate(ENGLISH_STOP_WORDS):\n",
    "    if stop_word not in whitelist:\n",
    "        modified_stopwords.append(stop_word)\n",
    "\n",
    "# POS tag sentences and lemmatize each word\n",
    "def tokenizer(text):\n",
    "    for token in wordpunct_tokenize(text):\n",
    "        if token not in modified_stopwords:\n",
    "            tag = tagger_mem(frozenset({token}))\n",
    "            yield lemmatize_mem(token, tags.get(tag[0][1], wn.NOUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline definition\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        ngram_range=(2, 3),\n",
    "        stop_words=modified_stopwords,\n",
    "        sublinear_tf=True,\n",
    "        min_df=0.0001,\n",
    "        max_df=0.3\n",
    "    )),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:   28.3s remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   28.5s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed:   28.7s remaining:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   28.7s remaining:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   28.7s remaining:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   28.8s remaining:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:   28.8s remaining:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   29.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   29.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[29681,    39],\n",
       "       [ 1501,   741]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validate using k-fold\n",
    "y_pred = cross_val_predict(\n",
    "    pipeline, df_data_1.get('tweet'),\n",
    "    y=df_data_1.get('label'),\n",
    "    cv=10, n_jobs=-1, verbose=20\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(df_data_1.get('label'), y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8b468a3c5682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   15.7s remaining:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   15.7s remaining:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   15.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   15.9s finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_SGD = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=modified_stopwords,\n",
    "        sublinear_tf=True,\n",
    "        min_df=0.0009\n",
    "    )),\n",
    "    ('classifier', SGDClassifier(\n",
    "        alpha=1e-4, n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Cross validate using k-fold\n",
    "y_pred_SGD = cross_val_predict(\n",
    "    pipeline_SGD, df_data_1.get('tweet'),\n",
    "    y=df_data_1.get('label'),\n",
    "    cv=10, n_jobs=-1, verbose=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29618,   102],\n",
       "       [ 1488,   754]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(df_data_1.get('label'), y_pred_SGD)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:   36.3s remaining:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   36.3s remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed:   36.5s remaining:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:   36.5s remaining:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   36.6s remaining:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   36.6s remaining:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:   36.6s remaining:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   36.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   36.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[29657,    63],\n",
       "       [ 1441,   801]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline definition\n",
    "pipeline_logistic = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=modified_stopwords,\n",
    "        sublinear_tf=True,\n",
    "        min_df=0.0001,\n",
    "        max_df=0.3\n",
    "    )),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Cross validate using k-fold\n",
    "y_pred_logistic = cross_val_predict(\n",
    "    pipeline_logistic, df_data_1.get('tweet'),\n",
    "    y=df_data_1.get('label'),\n",
    "    cv=10, n_jobs=-1, verbose=20\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(df_data_1.get('label'), y_pred_logistic)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = pipeline_logistic.fit(df_data_1.get('tweet'), df_data_1.get('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              tweet\n",
      "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
      "1  31964   @user #white #supremacists want everyone to s...\n",
      "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
      "3  31966  is the hp and the cursed child book up for res...\n",
      "4  31967    3rd #bihday to my amazing, hilarious #nephew...\n",
      "(17197, 2)\n"
     ]
    }
   ],
   "source": [
    "df_data_2 = pd.read_csv(get_object_storage_file_with_credentials_2659b32ff9a04774afc2ee0815088bcf('project1', 'test_tweets.csv'))\n",
    "print(df_data_2.head())\n",
    "\n",
    "print(df_data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logistic_model.predict(df_data_2.get('tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to put file to the IBM datascience folder\n",
    "def put_file(credentials, local_file_name): \n",
    "    \"\"\"This functions returns a StringIO object containing the file content from Bluemix Object Storage V3.\"\"\" \n",
    "    f = open(local_file_name,'r') \n",
    "    my_data = f.read() \n",
    "    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens']) \n",
    "    data = {'auth': {'identity': {'methods': ['password'], 'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']}, 'password': credentials['password']}}}}} \n",
    "    headers1 = {'Content-Type': 'application/json'} \n",
    "    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1) \n",
    "    resp1_body = resp1.json() \n",
    "    for e1 in resp1_body['token']['catalog']: \n",
    "        if(e1['type']=='object-store'): \n",
    "            for e2 in e1['endpoints']: \n",
    "                if(e2['interface']=='public'and e2['region']== credentials['region']):    \n",
    "                    url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name]) \n",
    "    \n",
    "    s_subject_token = resp1.headers['x-subject-token'] \n",
    "    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'} \n",
    "    resp2 = requests.put(url=url2, headers=headers2, data = my_data ) \n",
    "    print (resp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(predictions, columns = ['label'])\n",
    "\n",
    "df.to_csv('Dataset.csv',index=False)\n",
    "\n",
    "put_file(credentials_2661 ,'Dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
